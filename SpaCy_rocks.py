from __future__ import division, print_function, unicode_literals

# å®‰è£…æ¨¡å‹
# pip install zh_core_web_sm-2.0.5.tar.gz
# å†ä¸ºè¿™ä¸ªæ¨¡å‹å»ºç«‹ä¸€ä¸ªé“¾æ¥
# spacy link zh_core_web_sm zh
# ğŸ˜‚

# python -c "import os; path = os.sys.executable;folder=path[0 : path.rfind(os.sep)]; print (folder)"
# python -c "import spacy; print (spacy.__version__)"
# python -c "import os; import spacy; print(os.path.dirname(spacy.__file__))"
# python -c "import os; import zh_core_web_sm; print(os.path.dirname(zh_core_web_sm.__file__))"
# python -m spacy init-model zh /tmp/cc_zh_300_vec --vectors-loc cc.zh.300.vec.gz


import os, re, plac, random, json, time
import spacy
# nlpå¯¹è±¡è¦è¢«ç”¨æ¥åˆ›å»ºæ–‡æ¡£ï¼Œè®¿é—®è¯­è¨€æ³¨é‡Šå’Œä¸åŒçš„NLPå±æ€§ã€‚æˆ‘ä»¬é€šè¿‡åŠ è½½ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶æ¥åˆ›å»ºä¸€ä¸ªdocumentã€‚
# document = open(filename).read()
# doc = nlp(document)

from spacy import displacy
try:
	import zh_core_web_sm
	nlp = zh_core_web_sm.load()
	Loading_Path = "/home/wangdi498/anaconda3/envs/rasa/lib/python3.6/site-packages/zh_core_web_sm/zh_core_web_sm-2.0.5/"
	print ("\nInfo: The zh_core_web_sm module can be loaded.\n")
except (ModuleNotFoundError, IOError):
	print ("\nWarning! The zh_core_web_sm module cannot be loaded! Consequently BIO tagging, NER, word2vec, hashing, and parsing will not be launched either!\n")
	from spacy.lang.zh import Chinese
	nlp = Chinese()
	Loading_Path = "/home/wangdi498/anaconda3/envs/rasa/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-2.0.0/"
except:
	print ("\nError! Neither the en_core_web_sm nor the zh_core_web_sm module can be loaded!\n")

# å› ä¸ºzn_core_web_små·²ç»é”™è¯¯åœ°æŒ‡å®šäº†å¦‚ä½•åˆ†è¯ï¼Œæ‰€ä»¥åªèƒ½å¼ºè¡Œè®©jiebaåŠ è½½è‡ªå®šä¹‰å­—å…¸ã€‚é¢‘ç‡è¶Šé«˜ï¼Œæˆè¯çš„æ¦‚ç‡å°±è¶Šå¤§ã€‚
# https://github.com/fxsjy/jieba/issues/14
import jieba
customized_jieba_dict = "/home/wangdi498/SpaCy/customized_jieba_dict.txt"
jieba.load_userdict(customized_jieba_dict)


# Tokenæ˜¯è¯æˆ–æ ‡ç‚¹ï¼Œæ‰€ä»¥å…¶å±æ€§æœ‰attributesã€tagsã€dependenciesç­‰ç­‰ã€‚ Lexemeæ˜¯word typeï¼Œæ²¡æœ‰å†…å®¹ï¼Œæ‰€ä»¥å…¶å±æ€§æœ‰shapeã€stopã€flagsç­‰ç­‰ã€‚
# Docæ˜¯ä¸€äº›Tokençš„åºåˆ—ï¼ŒVocabæ˜¯ä¸€äº›Lexemeçš„åºåˆ—ï¼ŒSpanæ˜¯Docçš„ä¸€ä¸ªsliceï¼ŒStringStoreæ˜¯æŠŠhashå€¼æ˜ å°„æˆå­—ç¬¦ä¸²çš„å­—å…¸ã€‚ æ‰€ä»¥éå†docå¾—åˆ°tokenï¼Œéå†vocabå¾—åˆ°lexemeï¼Œlexeme=doc.vocab[token.text]ã€‚
# å¦‚æœDocæ˜¯nlp(u"2018å¹´9æœˆ27æ—¥")ï¼Œé‚£ä¹ˆSpan(doc,0,1)=''ï¼ŒSpan(doc,0,2)='2018'ï¼ŒSpan(doc,0,3)='2018å¹´'â€¦â€¦



def Tokenization():
	# .pos_åŒ…æ‹¬ï¼š NOUNã€ VERBã€ PRONã€ PROPNã€ ADJã€ ADVã€ ADPã€ DETã€ CCONJã€ SPACEã€ PARTã€ SYMã€ NUMã€ Xã€ INTJâ€¦â€¦
	# .tag_åŒ…æ‹¬ï¼š NNã€ NNSã€ VBPã€ JJã€ JJRã€ PRPã€ DTã€ INâ€¦â€¦
	# å…¶ä»–åˆ¤åˆ«åŒ…æ‹¬ï¼šis_alphaã€is_punctã€is_digitã€like_numã€like_emailâ€¦â€¦
	print ("\nThe outcomes of Tokenization are:")
	doc = nlp(u"äº¬ä¸œCEOåˆ˜å¼ºä¸œåœ¨ç¾å›½æ˜å°¼è‹è¾¾æ¶‰å«Œæ€§ä¾µå¥³å¤§å­¦ç”Ÿã€‚")
	for token in doc:
		print ("\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}".format(token.text, token.lemma_, token.ent_iob, token.ent_iob_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)) # B=3, I=1, O=2, ä¸­æ–‡æ²¡æœ‰Iã€‚
	# NLV = spacy.load("/tmp/cc_zh_300_vec")
	# doc = NLV(u"äº¬ä¸œCEOåˆ˜å¼ºä¸œåœ¨ç¾å›½æ˜å°¼è‹è¾¾æ¶‰å«Œæ€§ä¾µå¥³å¤§å­¦ç”Ÿã€‚")
	# for token in doc:
	# 	print ('\t', token.text)


def Tagging():
	print ("\nThe outcomes of Tagging are:")
	from spacy.symbols import ORTH, LEMMA, POS, TAG
	NLU = spacy.load('en_core_web_sm', parser=False, entity=False)
	# å¦‚æœloadäº†en_core_web_smï¼Œé‚£ä¹ˆåªèƒ½ç”¨add_special_caseæŒ‡å®šåˆ†è¯ï¼Œå› ä¸ºen_core_web_smæ— æ³•åšä¸­æ–‡åˆ†è¯ã€‚
	# å¦‚æœloadäº†zh_core_web_smï¼Œé‚£ä¹ˆadd_special_caseæ— æ•ˆï¼Œå› ä¸ºzn_core_web_små·²ç»é”™è¯¯åœ°æŒ‡å®šäº†å¦‚ä½•åˆ†è¯ã€‚
	special_case1 = [{ORTH: u'å¤§', LEMMA: u'å¤§', POS: u'ADJ'}, {ORTH: u'å­¦ç”Ÿ', LEMMA: u'ç”Ÿ', POS: u'NOUN'}]
	NLU.tokenizer.add_special_case(u'å¤§å­¦ç”Ÿ', special_case1)
	for shit in NLU(u"å¤§å­¦ç”Ÿ"):
		print ("\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}".format(shit.text, shit.lemma_, shit.pos_, shit.tag_, shit.dep_, shit.shape_, shit.is_alpha, shit.is_stop))
	special_case2 = [{ORTH: u'åˆ˜å¼ºä¸œ', LEMMA: u'åˆ˜å¼ºä¸œ', POS: u'PROPN'}, {ORTH: u'åˆ˜å¼ºä¸œï¼', LEMMA: u'åˆ˜å¼ºä¸œ', POS: u'PROPN'}]
	NLU.tokenizer.add_special_case(u'åˆ˜å¼ºä¸œ', [{ORTH: u'åˆ˜å¼ºä¸œ'}])
	for shit in NLU(u"OMGåˆ˜å¼ºä¸œ"):
		print ("\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}".format(shit.text, shit.lemma_, shit.pos_, shit.tag_, shit.dep_, shit.shape_, shit.is_alpha, shit.is_stop))

	from spacy.vocab import Vocab
	vocab = Vocab(strings=[u'åˆ˜å¼ºä¸œ'])
	from spacy.attrs import LEMMA
	print ("\nFuck!!! Tokenization has screwed up Tagging!")
	sentence = "æ˜¯åˆ˜å¼ºä¸œå®³äº†åˆ˜å¼ºä¸œè‡ªå·±ï¼"
	indexes = [p.span() for p in re.finditer('åˆ˜å¼ºä¸œ', sentence, flags=re.IGNORECASE)] # 'åˆ˜å¼ºä¸œ\w+'
	doc = nlp(sentence)
	print ("The indexes are: {}.".format(indexes))
	for start, end in indexes:
		doc.merge(start_idx=start, end_idx=end)
	for shit in doc:
		print ("\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}".format(shit.text, shit.lemma_, shit.pos_, shit.tag_, shit.dep_, shit.shape_, shit.is_alpha, shit.is_stop))

	# ç”±äºzh_core_web_smæœ¬èº«çš„åˆ†è¯é”™è¯¯ï¼Œä¸€æ—¦è°ƒç”¨nlpå°±ä¼šè§¦å‘pipelineã€å°±ä¼šé”™è¯¯åœ°åˆ†è¯ï¼Œæ‰€ä»¥åé¢ä¸è®ºæ€æ ·é‡æ–°ç»„è¯éƒ½åªèƒ½åŸºäºé”™è¯¯çš„åˆ†è¯ä¸Šï¼
	doc = nlp("æ˜¯åˆ˜å¼ºä¸œå®³äº†åˆ˜å¼ºä¸œè‡ªå·±ï¼")
	span = doc[1:4]
	with doc.retokenize() as retokenizer:
		retokenizer.merge(span, attrs={LEMMA: doc.vocab.strings[span.text]})
	for shit in doc:
		print ("\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}".format(shit.text, shit.lemma_, shit.pos_, shit.tag_, shit.dep_, shit.shape_, shit.is_alpha, shit.is_stop))

	# ç”±äºzh_core_web_smæœ¬èº«çš„åˆ†è¯é”™è¯¯ï¼Œä¸€æ—¦è°ƒç”¨nlpå°±ä¼šè§¦å‘pipelineã€å°±ä¼šé”™è¯¯åœ°åˆ†è¯ï¼Œæ‰€ä»¥åé¢ä¸è®ºæ€æ ·é‡æ–°ç»„è¯éƒ½åªèƒ½åŸºäºé”™è¯¯çš„åˆ†è¯ä¸Šï¼
	doc = nlp("æ˜¯åˆ˜å¼ºä¸œå®³äº†åˆ˜å¼ºä¸œè‡ªå·±ï¼")
	span = doc[1:4]
	lemma_id = doc.vocab.strings[span.text] # ç”¨IDæ¥ä»£è¡¨å­—ç¬¦ä¸²ï¼šstring_id = nlp.vocab.strings[string]
	span.merge(lemma=lemma_id)
	for shit in doc:
		print ("\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}".format(shit.text, shit.lemma_, shit.pos_, shit.tag_, shit.dep_, shit.shape_, shit.is_alpha, shit.is_stop))


def Hashing():
	print ("\nThe outcomes of Hashing are:")
	doc = nlp(u"äº¬ä¸œCEOåˆ˜å¼ºä¸œåœ¨ç¾å›½æ˜å°¼è‹è¾¾æ¶‰å«Œæ€§ä¾µå¥³å¤§å­¦ç”Ÿã€‚")
	apple = nlp.vocab.strings['apple'] # ç”¨IDæ¥ä»£è¡¨å­—ç¬¦ä¸²ï¼šstring_id = nlp.vocab.strings[string]
	try:
		assert nlp.vocab[apple] == nlp.vocab[u'apple'], "\nError! This word cannot be hashed!"
		print ("Info, the vocabulary can correspond to the ID %d." %apple)
	except:
		print ("Error! The vocabulary cannot correspond to the ID %d!" %apple)
	for word in doc:
		lexeme = doc.vocab[word.text]
		print ("\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}".format(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_, lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_))
	# word.orth_å’Œword.textæ˜¯ç­‰æ•ˆçš„ã€‚ORTHè¡¨ç¤ºtokençš„verbatimå€¼ï¼Œorth_æœ¬è´¨ä¸Šæ˜¯ç”¨__get__æ–¹æ³•è°ƒç”¨self.vocab.strings[self.c.lex.orth]ã€‚
	# Hasheså€¼ä¸èƒ½è¢«æ¢å¤æˆåŸè¯ã€‚
	print ("The hash of 'æ€§ä¾µ' is:", doc.vocab.strings[u'æ€§ä¾µ'])
	print ("The hash of 'CEO' is:", doc.vocab.strings[u'CEO'])
	# print ("The instance of '8011747669617544689' is:", doc.vocab.strings[8011747669617544689])


def Entity():
	print ("\nThe outcomes of Entity Extraction are:")
	doc = nlp(u"äº¬ä¸œCEOåˆ˜å¼ºä¸œåœ¨ç¾å›½æ˜å°¼è‹è¾¾æ¶‰å«Œæ€§ä¾µå¥³å¤§å­¦ç”Ÿã€‚")
	for ent in doc.ents:
		print ("\t{}\t\t{}\t{}\t{}\t{}\t{}".format(ent.text, ent.start_char, ent.end_char, ent.label_, doc[doc.ents.index(ent)].ent_iob_, doc[doc.ents.index(ent)].ent_type_))

	from spacy.tokens import Span
	doc = nlp(u"å¥¶èŒ¶å¦¹å¦¹é‡è§VPå°±æœ‰90%çš„å‡ ç‡1ä½å‡ºé“â€¦â€¦")
	for ent in doc.ents:
		print ("\t{}\t\t{}\t{}\t{}\t{}\t{}".format(ent.text, ent.start_char, ent.end_char, ent.label_, doc[doc.ents.index(ent)].ent_iob_, doc[doc.ents.index(ent)].ent_type_))
	augment = [Span(doc, 0, 1, label=doc.vocab.strings[u'WORK_OF_ART'])]
	doc.ents = list(doc.ents) + augment
	for ent in doc.ents:
		print ("\t{}\t\t{}\t{}\t{}\t{}\t{}".format(ent.text, ent.start_char, ent.end_char, ent.label_, doc[doc.ents.index(ent)].ent_iob_, doc[doc.ents.index(ent)].ent_type_))


def Vectorization1():
	print ("\nThe outcomes of Vectorization1 are:")
	tokens = nlp(u"äº¬ä¸œåˆ˜å¼ºä¸œåœ¨æŸä¸ªæ®è¯´çš„æ—¶é—´å’Œä¸€ä¸ªä¼ è¯´çš„åœ°ç‚¹é‡è§å¥¶èŒ¶å¦¹å¦¹ã€‚")
	for token in tokens:
		print ("\t{}\t{}\t{}\t{}".format(token.text, token.has_vector, token.vector_norm, token.is_oov))
		if token.text == 'å¥¶èŒ¶':
			print ("\tThe contents of token '{}' is {} with length {}.".format(token.text, token.vector, token.vector.size))
def Vectorization2():
	print ("\nThe outcomes of Vectorization2 are:")
	# python -m spacy init-model zh /tmp/cc_zh_300_vec --vectors-loc cc.zh.300.vec.gz
	# nlp.vocab.vectors.from_glove('...')
	NLV = spacy.load("/tmp/cc_zh_300_vec")
	tokens = NLV(u"äº¬ä¸œåˆ˜å¼ºä¸œåœ¨æŸä¸ªæ®è¯´çš„æ—¶é—´å’Œä¸€ä¸ªä¼ è¯´çš„åœ°ç‚¹é‡è§å¥¶èŒ¶å¦¹å¦¹ã€‚")
	for token in tokens:
		print ("\t{}\t{}\t{}\t{}".format(token.text, token.has_vector, token.vector_norm, token.is_oov))
		if token.text == 'å¥¶èŒ¶':
			print ("\tThe contents of token '{}' is {} with length {}.".format(token.text, token.vector, token.vector.size))


def Similarity1():
	print ("\nThe outcomes of Similarity1 are:")
	tokens = nlp(u"å“¥å“¥å¦¹å¦¹ç”·äººå¥³äººç å†œæ˜¥å¤ç§‹å†¬å­£èŠ‚æˆ‘ä½ ä»–") # Similarityæ˜¯æ¯”è¾ƒè¯åµŒå…¥æˆ–è¯å‘é‡çš„ç»“æœã€‚
	for token1 in tokens:
		for token2 in tokens:
			print ("\t{} <=> {} : {}".format(token1.text, token2.text, token1.similarity(token2)))
def Similarity2():
	print ("\nThe outcomes of Similarity2 are:")
	NLV = spacy.load("/tmp/cc_zh_300_vec")
	tokens = NLV(u"å“¥å“¥å¦¹å¦¹ç”·äººå¥³äººç å†œæ˜¥å¤ç§‹å†¬å­£èŠ‚æˆ‘ä½ ä»–") # Similarityæ˜¯æ¯”è¾ƒè¯åµŒå…¥æˆ–è¯å‘é‡çš„ç»“æœã€‚
	for token1 in tokens:
		for token2 in tokens:
			print ("\t{} <=> {} : {}".format(token1.text, token2.text, token1.similarity(token2)))


def Stop():
	print ("\nThe outcomes of Stop Words are:")
	from spacy.lang.en.stop_words import STOP_WORDS
	# print (STOP_WORDS)
	STOP_WORDS.add("your_additional_stop_word_here")
	for word in STOP_WORDS:
		lexeme = nlp.vocab[word]
		lexeme.is_stop = True
		# print (lexeme.text)

	nlp.Defaults.stop_words |= {"äº†", "å•Š", "å§", "å—¯"} # å•ä¸ªè¯å¯ä»¥ç›´æ¥.add()
	nlp.Defaults.stop_words -= {"å—¯"} # å•ä¸ªè¯å¯ä»¥ç›´æ¥.remove()
	for word in nlp.Defaults.stop_words:
		lexeme = nlp.vocab[word]
		lexeme.is_stop = True
		# print (lexeme.text)
	print (nlp.Defaults.stop_words)


def Parsing():
	print ("\nThe outcomes of Parsing are:")

	# ä¸­æ–‡æ— æ³•ç›´æ¥åˆ†å¥ï¼
	paragraph = nlp(u"äº¬ä¸œCEOåˆ˜å¼ºä¸œ, åœ¨ç¾å›½æ˜å°¼è‹è¾¾ï¼Œæ¶‰å«Œæ€§ä¾µå¥³å¤§å­¦ç”Ÿ. å¥¶èŒ¶å¦¹å¦¹ã€‚é‡è§VPå°±æœ‰90%çš„å‡ ç‡1ä½å‡ºé“â€¦â€¦")
	for sent in paragraph.sents:
		print("\t{}".format(sent.text))

	# è‡ªå®šä¹‰ä¸­æ–‡åˆ†å¥ã€‚
	from spacy.pipeline import SentenceSegmenter
	def split_on_punctuation(doc):
		start = 0
		whether_segmenter = False
		for word in doc:
			if whether_segmenter and not word.is_space:
				yield doc[start:word.i]
				start = word.i
				whether_segmenter = False
			elif word.text in ",.:;?!ï¼Œã€‚ï¼šï¼›ï¼Ÿï¼":
				whether_segmenter = True
		if start < len(doc):
			yield doc[start:len(doc)]
	punctuation = re.compile(r",.:;?!ï¼Œã€‚ï¼šï¼›ï¼Ÿï¼")
	SS = SentenceSegmenter(nlp.vocab, strategy=split_on_punctuation)
	nlp.add_pipe(SS)
	paragraph = nlp(u"äº¬ä¸œCEOåˆ˜å¼ºä¸œ, åœ¨ç¾å›½æ˜å°¼è‹è¾¾ï¼Œæ¶‰å«Œæ€§ä¾µå¥³å¤§å­¦ç”Ÿ. å¥¶èŒ¶å¦¹å¦¹â€¦â€¦é‡è§VPå°±æœ‰90%çš„å‡ ç‡1ä½å‡ºé“ã€‚")
	for sent in paragraph.sents:
		print("\t{}".format(sent.text))

	print ("\n")
	sentence = nlp("äº¬ä¸œCEOåˆ˜å¼ºä¸œåœ¨ç¾å›½æ˜å°¼è‹è¾¾æ¶‰å«Œæ€§ä¾µå¥³å¤§å­¦ç”Ÿã€‚")
	for word in sentence:
		print("\t{}: {}".format(word, str(list(word.children))))


def Serialization():
	print ("\nThe outcomes of Serialization are:")
	try:
		text = open("/home/wangdi498/SpaCy/diary2.txt", 'r').read() # 'r'ä¼šæŒ‰ç¼–ç æ ¼å¼è¿›è¡Œè§£æï¼Œread()è¿”å›çš„æ˜¯strï¼›'rb'ï¼šä¼šæŒ‰äºŒè¿›åˆ¶è¿›è¡Œè§£æï¼Œread()è¿”å›çš„æ˜¯bytesã€‚
		print ("\nInfo: The Serialization file can be read.\n")
	except FileNotFoundError:
		print ("\nError! The Serialization file cannot be read!\n")
		sys.exit(0) # os._exit()ä¼šç›´æ¥å°†pythonç¨‹åºç»ˆæ­¢ï¼Œä¹‹åçš„æ‰€æœ‰ä»£ç éƒ½ä¸ä¼šç»§ç»­æ‰§è¡Œã€‚
	except:
		print ("\nError! The .txt file must be UTF-8 encoded format!\n")
	doc = nlp(text)
	doc.to_disk("/home/wangdi498/SpaCy/diary1.bin")

	from spacy.tokens import Doc
	from spacy.vocab import Vocab
	doc = Doc(Vocab()).from_disk("/home/wangdi498/SpaCy/diary1.bin")
	print ("The texts are:\n{}".format(doc))
	
	from spacy.tokens import Span
	doc = nlp(text)
	print ("\nThe 1st round of Save and Load is:")
	for ent in doc.ents:
		print ("\t{}\t\t{}\t{}\t{}\t{}\t{}".format(ent.text, ent.start_char, ent.end_char, ent.label_, doc[doc.ents.index(ent)].ent_iob_, doc[doc.ents.index(ent)].ent_type_))
	assert len(doc.ents) != 0, "\nError! This document cannot be empty!" # é˜²æ­¢Docä¸ºç©ºã€‚
	augment = [Span(doc, 0, 2, label=doc.vocab.strings[u'EVENT'])]
	doc.ents = list(doc.ents) + augment
	doc.to_disk("/home/wangdi498/SpaCy/diary2.bin")
	print ("\nThe 2nd round of Save and Load is:")
	for ent in doc.ents:
		print ("\t{}\t\t{}\t{}\t{}\t{}\t{}".format(ent.text, ent.start_char, ent.end_char, ent.label_, doc[doc.ents.index(ent)].ent_iob_, doc[doc.ents.index(ent)].ent_type_))

	paragraph = Doc(Vocab()).from_disk("/home/wangdi498/SpaCy/diary2.bin")
	assert len(paragraph.ents) != 0, "\nError! This document cannot be empty!" # é˜²æ­¢Docä¸ºç©ºã€‚
	print ("\nThe 3rd round of Save and Load is:")
	for ent in paragraph.ents:
		print ("\t{}\t\t{}\t{}\t{}\t{}\t{}".format(ent.text, ent.start_char, ent.end_char, ent.label_, doc[doc.ents.index(ent)].ent_iob_, doc[doc.ents.index(ent)].ent_type_))
	assert [(ent.text, ent.label_) for ent in paragraph.ents] != [(u'2018å¹´9æœˆ27æ—¥', u'EVENT')], "\nHere! The entity '%s' has matched the specified one." %ent.text


def Pipeline():
	print ("\nThe outcomes of Pipelining are:")
	lang = 'zh'
	pipeflow = ['tagger', 'parser', 'ner']
	cls = spacy.util.get_lang_class(lang)
	nlp = cls()
	for name in pipeflow:
		component = nlp.create_pipe(name)
		nlp.add_pipe(component) # åªæœ‰è°ƒç”¨nlpæ‰èƒ½è§¦å‘pipelineï¼Œæ‰èƒ½ä¾æ¬¡è¿è¡Œdummy_componentã€taggerã€ parserã€ nerï¼
	print ("The model has been saved into the disk.")
	nlp.from_disk(Loading_Path)

	doc = nlp.make_doc(u"äº¬ä¸œCEOåˆ˜å¼ºä¸œåœ¨ç¾å›½æ˜å°¼è‹è¾¾æ¶‰å«Œæ€§ä¾µå¥³å¤§å­¦ç”Ÿã€‚")
	for name, proc in nlp.pipeline:
		print ("\tHere the component is: {}\t{}".format(name, proc))
		doc = proc(doc)
		# print ("The pipeline is:", nlp.pipeline)
	print ("\n")
	print ("Before adding a component, the pipes are:", nlp.pipe_names)

	def dummy_component(doc):
		print ("Since this component is called, this doc '%s' has %d tokens." %(doc, len(doc)))
		return doc
	nlp.add_pipe(dummy_component, name='dummy_info', first=True)
	print ("After adding a component, the pipes are:", nlp.pipe_names)
	doc = nlp(u"äº¬ä¸œåˆ˜å¼ºä¸œåœ¨æŸä¸ªæ®è¯´çš„æ—¶é—´å’Œä¸€ä¸ªä¼ è¯´çš„åœ°ç‚¹é‡è§å¥¶èŒ¶å¦¹å¦¹ã€‚") # åªæœ‰è°ƒç”¨nlpæ‰èƒ½è§¦å‘pipelineï¼Œæ‰èƒ½ä¾æ¬¡è¿è¡Œdummy_componentã€taggerã€ parserã€ nerï¼



def convert_JSON_python(source_file = "/home/wangdi498/SpaCy/NER_example2.json"):
	def store(target_file, data):
		with open(target_file, 'w') as json_file:
			json_file.write(json.dumps(data))
	def load(source_file):
		with open(source_file) as json_file:
			data = json.load(json_file)
			return data
	data = load(source_file)
	try:
		result = []
		for example in data['rasa_nlu_data']['common_examples']:
			marking_dict = {}
			marking_list = []
			for entity in example['entities']:
				marking_list.append((entity['start'], entity['end'], entity['entity']))
				marking_dict['entities'] = marking_list
			result.append((example['text'], marking_dict))
	except (IndexError, IOError) as e:
		print ("\nError! Incorrect data! {}".format(e))
	else:
		print ("\nThe convertion result is\n{}".format(result))
		return result


# GoldParse objectæ˜¯Doc objectçš„å®ä¾‹åŒ–ï¼Œæ”¶é›†è®­ç»ƒæ•°æ®ä½œä¸ºgold standardï¼Œç„¶åç¼–ç æ ‡æ³¨ã€‚æ‰€ä»¥Docçš„å†…å®¹æ˜¯textï¼Œè€ŒGoldParseçš„å†…å®¹æ˜¯labelã€‚
def Train():
	print ("\nThe outcomes of Training and Updating are:")
	from spacy.tokens import Doc
	from spacy.vocab import Vocab
	from spacy.gold import GoldParse
	vocab = Vocab(tag_map={'N': {'pos': 'NOUN'}, 'V': {'pos': 'VERB'}})
	doc = Doc(vocab, words=['ç”¨æˆ·', 'ä½“éªŒ', 'APP'])
	gold = GoldParse(doc, tags=['N', 'V', 'N'])
	doc = Doc(Vocab(), words=['é™†é‡‘æ‰€', 'æˆç«‹', 'AIå®éªŒå®¤', 'å·²ç»', 'ä¸€å¹´'])
	gold = GoldParse(doc, entities=['U-ORG', 'O', 'U-TECHNOLOGY', 'O', 'U-DATE'])
	doc = Doc(nlp.vocab, words=[u'åˆ˜å¼ºä¸œ', u'ç« æ³½å¤©', u'å¤§å­¦ç”Ÿ', u'é‡è§'], spaces=[False, False, False, False])
	gold = GoldParse(doc, entities=[u'PERSON', u'PERSON', u'PRODUCT', u'O'])

	train_data = convert_JSON_python('/home/wangdi498/SpaCy/NER_example2.json')
	with nlp.disable_pipes(*[pipe for pipe in nlp.pipe_names if pipe != 'ner']):
		optimizer = nlp.begin_training()
		for i in range(10):
			random.shuffle(train_data)
			# æ¯è½®éƒ½ä¼šshuffleè®­ç»ƒæ•°æ®ï¼Œä¿è¯æ¨¡å‹ä¸ä¼šæ ¹æ®è®­ç»ƒé¡ºåºæ¥åšgeneralizationsã€‚ä¹Ÿå¯ä»¥è®¾ç½®dropout rateè®©æ¨¡å‹ä»¥ä¸€å®šå‡ ç‡æ”¾å¼ƒä¸€äº›featureså’Œrepresentationsæ¥é¿å…æ¨¡å‹è¿‡ç‰¢åœ°è®°ä½è®­ç»ƒæ•°æ®ã€‚
			for text, annotations in train_data:
				# doc = nlp.make_doc(text)
				# gold = GoldParse(doc, entities=entity_offsets)
				# nlp.update([doc], [gold], drop=0.5, sgd=optimizer)
				nlp.update([text], [annotations], sgd=optimizer) # ç”¨å¾—åˆ°çš„æ•°æ®æ›´æ–°æ¨¡å‹ã€‚
	nlp.to_disk("/home/wangdi498/SpaCy/models")



from pathlib import Path

TRAIN_DATA = [
    ('Who is Shaka Khan?', {
        'entities': [(7, 17, 'PERSON')]
    }),
    ('I like London and Berlin.', {
        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]
    })
]

TRAIN_DATA = convert_JSON_python("/home/wangdi498/_rasa_chatbot_vip/data/invest_nlu_data_train.json")
TEST_DATA = convert_JSON_python("/home/wangdi498/_rasa_chatbot_vip/data/invest_nlu_data_test.json")

@plac.annotations(
	model = ("Language model", "option", "m", str),
	output_dir = ("Output directory", "option", "o", Path),
	number_iterations = ("Number of training iterations", "option", "n", int))

def DIDA_NER(model=None, output_dir="/home/wangdi498/SpaCy/models", number_iterations=50):
	if model is not None:
		nlp = spacy.load(model)
		print ("In NER the goddamn language model {} already exists so it will be loaded.".format(model))
	else:
		nlp = spacy.blank('zh')
		print ("In NER the goddamn language model does not exist so 'zh_core_web_sm' will be loaded.")
	if 'ner' not in nlp.pipe_names:
		ner = nlp.create_pipe('ner') # create_pipe()åªå¯¹SpaCyæ‰¿è®¤çš„componentæœ‰æ•ˆã€‚
		nlp.add_pipe(ner, last=True)
	else:
		ner = nlp.get_pipe('ner')

	for _, annotations in TRAIN_DATA:
		for ent in annotations.get('entities'): # getå¯ä»¥è¿”å›Noneã€‚
			ner.add_label(ent[2])

	# å…¶ä½™componentå¿…é¡»åœ¨NERè®­ç»ƒæ—¶è¢«ç»ˆæ­¢ï¼
	other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
	with nlp.disable_pipes(*other_pipes):
		optimizer = nlp.begin_training()
		for iteration in range(number_iterations):
			random.shuffle(TRAIN_DATA)
			# æ¯è½®éƒ½ä¼šshuffleè®­ç»ƒæ•°æ®ï¼Œä¿è¯æ¨¡å‹ä¸ä¼šæ ¹æ®è®­ç»ƒé¡ºåºæ¥åšgeneralizationsã€‚ä¹Ÿå¯ä»¥è®¾ç½®dropout rateè®©æ¨¡å‹ä»¥ä¸€å®šå‡ ç‡æ”¾å¼ƒä¸€äº›featureså’Œrepresentationsæ¥é¿å…æ¨¡å‹è¿‡ç‰¢åœ°è®°ä½è®­ç»ƒæ•°æ®ã€‚
			losses = {}
			for text, annotations in TRAIN_DATA:
				nlp.update([text], [annotations], drop=0.5, sgd=optimizer, losses=losses) # ç”¨å¾—åˆ°çš„æ•°æ®æ›´æ–°æ¨¡å‹ã€‚
			print ("In round {} of {} NER the loss is {}.".format(iteration, number_iterations, losses))

	# å§‘ä¸”ç”¨è®­ç»ƒæ•°æ®ç›´æ¥æµ‹è¯•ã€‚
	for text, _ in TRAIN_DATA:
		doc = nlp(text)
		print ("\n{}".format([(ent.text, ent.label_) for ent in doc.ents]))
		# print ("\nHow can I trust the entities without test?!\n{}".format([(ent.text, ent.label_) for ent in doc.ents]))
		# print ("\nHow can I trust the tokens without test?!\n{}".format([(token.text, token.ent_type_, token.ent_iob_) for token in doc]))

    # å­˜å‚¨è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚
	if output_dir is not None:
		output_dir = Path(output_dir)
		if not output_dir.exists():
			output_dir.mkdir()
		nlp.to_disk(output_dir)
		print ("\nThe NER model is saved to {}. This is the end of training.".format(output_dir))

		# è¯»å–è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚
		print ("\nThe NER model is loaded from {}. This is the beginning of testing.".format(output_dir))
		nlu = spacy.load(output_dir)
		for text, _ in TEST_DATA:
			doc = nlu(text)
			print ("\n{}".format([(ent.text, ent.label_) for ent in doc.ents]))
			# print ("\nI can trust the entities with test.\n{}".format([(ent.text, ent.label_) for ent in doc.ents]))
			# print ("\nI can trust the tokens with test.\n{}".format([(token.text, token.ent_type_, token.ent_iob_) for token in doc]))



### $$$ %%% ===

if __name__ == "__main__":
	print ("\nHere we go...\n")
	# Tokenization()
	# Tagging()
	# Hashing()
	# Entity()
	# Vectorization1() # 128ç»´è¯ç©ºé—´ï¼Œ|å¥¶èŒ¶|=23.189319610595703ã€‚
	# Vectorization2() # 300ç»´è¯ç©ºé—´ï¼Œ|å¥¶èŒ¶|=2.2992143630981445ã€‚
	# Similarity1()
	# Similarity2()
	# Stop()
	# Parsing()
	# Serialization()
	# Pipeline()
	# Train()
	plac.call(DIDA_NER)
	print ("\nDone!\n")
